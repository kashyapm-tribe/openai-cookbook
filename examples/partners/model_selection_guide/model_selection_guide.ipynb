{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5e91602",
   "metadata": {},
   "source": [
    "# Practical Guide for Model Selection for Real‑World Use Cases\n",
    "\n",
    "## Purpose & Audience\n",
    "\n",
    "This cookbook serves as your practical guide to selecting, prompting, and deploying the right OpenAI model (between GPT 4.1, o3, and o4-mini) for specific workloads. Instead of exhaustive documentation, we provide actionable decision frameworks and real-world examples that help Solutions Engineers, Technical Account Managers, Partner Architects, and semi-technical practitioners quickly build working solutions. The content focuses on current model capabilities, vertical-specific implementations, and today's industry needs, with clear pathways from model selection to production deployment. Each section offers concise, adaptable code examples that you can immediately apply to your use cases while pointing to existing resources for deeper dives into specific topics.\n",
    "\n",
    "> Note: The below prescriptive guidance and experimentation has been conducted with latest SOTA models available today. These metrics are bound to change in the future with different scenarios and timeline into consideration.\n",
    "\n",
    "## How to Use This Cookbook\n",
    "\n",
    "This cookbook is organized into distinct sections to help you quickly find the information you need. Each section covers a specific aspect of model selection, implementation, and deployment.\n",
    "\n",
    "1. **[Purpose & Audience](#purpose-audience)**: An overview of who this cookbook is for and what it covers.\n",
    "2. **[Model Guide](#model-guide)**: A quick reference to help you select the right model for your needs, including model comparisons and evolution diagrams based on mapping different use-case scenarios.\n",
    "3. **Use Cases**:\n",
    "   - **[3A. Long-Context RAG for Legal Q&A](#3a-use-case-long-context-rag-for-legal-qa)**: Building an agentic system to answer questions from complex legal documents.\n",
    "   - **[3B. AI Co-Scientist for Pharma R&D](#3b-use-case-ai-co-scientist-for-pharma-rd)**: Accelerating experimental design in pharmaceutical research with multi-agent systems.\n",
    "   - **[3C. Insurance Claim Processing](#3c-use-case-insurance-claim-processing)**: Digitizing and validating handwritten insurance forms with vision and reasoning.\n",
    "4. **[Prototype to Production](#prototype-to-production)**: A checklist to help you transition from prototype to production.\n",
    "5. **[Adaptation Decision Tree](#adaptation-decision-tree)**: A flowchart to guide your model selection based on specific requirements.\n",
    "6. **[Appendices](#appendices)**: Reference materials including pricing, latency, prompt patterns, and links to external resources.\n",
    "\n",
    "For quick decisions, focus on the Model Guide and Adaptation Decision Tree sections. For implementation details, explore the specific use cases relevant to your needs.\n",
    "\n",
    "\n",
    "================================================================================\n",
    "\n",
    "\n",
    "\n",
    "## Model Guide\n",
    "\n",
    "## 2.1 Model‑Intro Matrix\n",
    "\n",
    "| Model | Core strength | Ideal first reach‑for | Watch‑outs | Escalate / Downgrade path |\n",
    "| :---- | :---- | :---- | :---- | :---- |\n",
    "| GPT‑4o | Real‑time voice / vision chat | Live multimodal agents | Slightly below 4.1 on text SOTA (state-of-the-art) | Need deep reasoning → o4‑mini |\n",
    "| GPT‑4.1 | 1 M‑token text accuracy king | Long‑doc analytics, code review | Cannot natively reason; higher cost than minis | Tight budget → 4.1‑mini / nano |\n",
    "| o3 | Deep tool‑using agent | High‑stakes, multi‑step reasoning | Latency & price | Cost/latency → o4‑mini |\n",
    "| o4‑mini | Cheap, fast reasoning | High‑volume \"good‑enough\" logic | Depth ceiling vs o3 | Accuracy critical → o3 |\n",
    "\n",
    "*(Full price and utility table → [Section 6.1](#appendices))*\n",
    "\n",
    "## 2.2 Model Evolution at a Glance\n",
    "\n",
    "OpenAI's model lineup has evolved to address specialized needs across different dimensions. These diagrams showcase the current model families and their relationships.\n",
    "\n",
    "### Fundamental Differences: \"o-series\" vs \"GPT\" Models\n",
    "\n",
    "OpenAI offers two distinct model families, each with unique strengths:\n",
    "\n",
    "- **GPT Models (4o, 4.1)**: Optimized for general-purpose tasks with excellent instruction following. GPT-4.1 excels with long contexts (1M tokens) while GPT-4o has variants for realtime speech, text-to-speech, and speech-to-text. GPT-4.1 also comes in a mini, and nano variant, while GPT-4o has a mini variant. These variants are cheaper and faster than their full-size counterparts.\n",
    "\n",
    "- **o-series Models (o3, o4-mini)**: Specialized for deep reasoning and step-by-step problem solving. These models excel at complex, multi-stage tasks requiring logical thinking and tool use. Choose these when accuracy and reasoning depth are paramount. These models also have an optional `reasoning_effort` parameter (that can be set to `low`, `medium`, or `high`), which allows users to control the amount of tokens used for reasoning.\n",
    "\n",
    "### OpenAI Model Evolution \n",
    "\n",
    "![OpenAI Model Evolution](../../../images/2.2_model_evolution.png)\n",
    "\n",
    "### Key Characteristics\n",
    "\n",
    "- **GPT-4.1 Family**: Optimized for long context processing with 1M token context window.\n",
    "- **o3**: Specialized for deep multi-step reasoning. \n",
    "- **o4-mini**: Combines reasoning capabilities with vision at lower cost.\n",
    "\n",
    "Each model excels in different scenarios, with complementary strengths that can be combined for complex workflows.\n",
    "\n",
    "In this cookbook we only experimented with the GPT-4.1 series models, o3, and o4-mini. We didn't experiment with the GPT-4o series models.\n",
    "\n",
    "================================================================================\n",
    "\n",
    "\n",
    "\n",
    "## 3A. Use Case: Long-Context RAG for Legal Q&A\n",
    "\n",
    "![Long-Context RAG for Legal Q&A](../../../images/3A_rag_task_card.png)\n",
    "## 🗂️ TL;DR Matrix\n",
    "\n",
    "This table summarizes the core technology choices and their rationale for **this specific Long-Context Agentic RAG implementation**.\n",
    "\n",
    "| Layer | Choice | Utility |\n",
    "| :---- | :---- | :---- |\n",
    "| **Chunking** | Sentence-aware Splitter | Splits document into 20 equal chunks, respecting sentence boundaries. |\n",
    "| **Routing** | `gpt-4.1-mini` | Uses natural language understanding to identify relevant chunks without embedding index. |\n",
    "| **Path Selection** | `select(ids=[...])` and `scratchpad(text=\"...\")` | Records reasoning while drilling down through document hierarchy. |\n",
    "| **Citation** | Paragraph-level | Balances precision with cost; provides meaningful context for answers. |\n",
    "| **Synthesis** | `gpt-4.1` (Structured Output) | Generates answers directly from selected paragraphs with citations. |\n",
    "| **Verification** | `o4-mini` (LLM-as-Judge) | Validates factual accuracy and citation correctness. |\n",
    "\n",
    "*Note: Prices and model identifiers accurate as of April 2025, subject to change.*\n",
    "\n",
    "This section outlines the construction of a Retrieval-Augmented Generation (RAG) system designed to accurately answer questions about complex and lengthy procedural texts, using the *Trademark Trial and Appeal Board Manual of Procedure (TBMP)* as a representative case. The TBMP is an essential legal resource detailing the procedures governing trademark litigation before the USPTO's Trademark Trial and Appeal Board, and is frequently consulted by intellectual property attorneys and legal professionals. By leveraging the latest OpenAI models, the system enhances understanding and interpretability of dense legal content, enabling precise, contextually aware responses through advanced language understanding and dynamic retrieval capabilities.\n",
    "\n",
    "These approaches can also be applied to other use cases that require precise information retrieval from complex documentation, such as healthcare compliance manuals, financial regulatory frameworks, or technical documentation systems where accuracy, citation, and auditability are mission-critical requirements.\n",
    "\n",
    "## 1\\. Scenario Snapshot\n",
    "\n",
    "* **Corpus:** The primary document is the [Trademark Trial and Appeal Board Manual of Procedure (TBMP, 2019 version)](https://www.uspto.gov/sites/default/files/documents/tbmp-2019.pdf). This manual contains detailed procedural rules and guidelines, coming to 1194 pages total.  \n",
    "* **Users:** The target users are intellectual property (IP) litigation associates and paralegals who need quick, accurate answers to procedural questions based *only* on the TBMP.  \n",
    "* **Typical Asks:** Users pose questions requiring synthesis and citation, such as:  \n",
    "  1. \"What are the requirements for filing a motion to compel discovery according to the TBMP?\"  \n",
    "  2. \"What deadlines apply to discovery conferences as specified in the manual?\"  \n",
    "  3. \"Explain how the Board handles claims of attorney-client privilege during depositions according to the TBMP.\"  \n",
    "  4. \"Enumerate the Fed. R. Civ. P. 11 sanctions the Board can invoke according to the TBMP.\"  \n",
    "\n",
    "*Note: Depending on your specific deployment environment, you may need to adapt some implementation steps to match your infrastructure requirements.*\n",
    "\n",
    "> While OpenAI's File Search tool offers a good starting point for many use cases, this section introduces a different approach that takes advantage of million-token context windows to process large documents without any preprocessing or vector database. The agentic approach described here enables zero-latency ingestion, dynamic granularity of retrieval, and fine-grained citation traceability.\n",
    "\n",
    "## 2\\. Agentic RAG Flow\n",
    "\n",
    "Before diving into the implementation, let's understand the overall approach:\n",
    "\n",
    "1. **Load the entire document** into the context window\n",
    "2. **Split into 20 chunks** that respect sentence boundaries\n",
    "3. **Ask the model** which chunks might contain relevant information\n",
    "4. **Drill down** into selected chunks by splitting them further\n",
    "5. **Repeat** until we reach paragraph-level content\n",
    "6. **Generate an answer** based on the selected paragraphs\n",
    "7. **Verify the answer** for factual accuracy\n",
    "\n",
    "This hierarchical navigation approach mimics how a human might skim a document, focus on relevant chapters, then specific sections, and finally read only the most relevant paragraphs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9bad1b",
   "metadata": {},
   "source": [
    "![Hierarchical Router](../../../images/3A_rag_hierarchical_router.png)\n",
    "\n",
    "\n",
    "## Agentic RAG System: Model Usage\n",
    "\n",
    "| Process Stage | Model Used | Purpose |\n",
    "|---------------|------------|---------|\n",
    "| Initial Routing | `gpt-4.1-mini` | Identifies which document chunks might contain relevant information |\n",
    "| Hierarchical Navigation | `gpt-4.1-mini` | Continues drilling down to find most relevant paragraphs |\n",
    "| Answer Generation | `gpt-4.1` | Creates structured response with citations from selected paragraphs |\n",
    "| Answer Verification | `o4-mini` | Validates factual accuracy and proper citation usage |\n",
    "\n",
    "This zero-preprocessing approach leverages large context windows to navigate documents on-the-fly, mimicking how a human would skim a document to find relevant information. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df87f0ac",
   "metadata": {},
   "source": [
    "## 3\\. Implementation\n",
    "\n",
    "Let's implement this approach step by step.\n",
    "\n",
    "Start by installing the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c78cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tiktoken pypdf nltk openai pydantic --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1d7d60",
   "metadata": {},
   "source": [
    "### 3.1 Document Loading\n",
    "\n",
    "First, let's load the document and check its size. For this guide, we'll focus on sections 100-900, which cover the core procedural aspects through Review of Decision of Board. Sections 1000 and beyond (Interferences, Concurrent Use Proceedings, Ex Parte Appeals) are specialized procedures outside our current scope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5fb149",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from io import BytesIO\n",
    "from pypdf import PdfReader\n",
    "import re\n",
    "import tiktoken\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# Download nltk data if not already present\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "def load_document(url: str) -> str:\n",
    "    \"\"\"Load a document from a URL and return its text content.\"\"\"\n",
    "    print(f\"Downloading document from {url}...\")\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    pdf_bytes = BytesIO(response.content)\n",
    "    pdf_reader = PdfReader(pdf_bytes)\n",
    "    \n",
    "    full_text = \"\"\n",
    "    \n",
    "\n",
    "    max_page = 920  # Page cutoff before section 1000 (Interferences)\n",
    "    for i, page in enumerate(pdf_reader.pages):\n",
    "        if i >= max_page:\n",
    "            break\n",
    "        full_text += page.extract_text() + \"\\n\"\n",
    "    \n",
    "    # Count words and tokens\n",
    "    word_count = len(re.findall(r'\\b\\w+\\b', full_text))\n",
    "    \n",
    "    tokenizer = tiktoken.get_encoding(\"o200k_base\")\n",
    "    token_count = len(tokenizer.encode(full_text))\n",
    "    \n",
    "    print(f\"Document loaded: {len(pdf_reader.pages)} pages, {word_count} words, {token_count} tokens\")\n",
    "    return full_text\n",
    "\n",
    "# Load the document\n",
    "tbmp_url = \"https://www.uspto.gov/sites/default/files/documents/tbmp-Master-June2024.pdf\"\n",
    "document_text = load_document(tbmp_url)\n",
    "\n",
    "# Show the first 500 characters\n",
    "print(\"\\nDocument preview (first 500 chars):\")\n",
    "print(\"-\" * 50)\n",
    "print(document_text[:500])\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf86c84",
   "metadata": {},
   "source": [
    "We can see that the document is over 900k tokens long! While we could fit that into GPT 4.1's context length, we also want to have verifiable citations, so we're going to proceed with a recursive chunking strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445cbcaa",
   "metadata": {},
   "source": [
    "### 3.2 Improved 20-Chunk Splitter with Minimum Token Size\n",
    "\n",
    "Now, let's create an improved function to split the document into 20 chunks, ensuring each has a minimum token size and respecting sentence boundaries.\n",
    "\n",
    "> 20 is an empirically chosen number for this specific document/task and it might need tuning for other documents based on size and structure (The higher the number, the more fine-grained the chunks). The key principle here however is splitting sections of the document up, in order to let the language model decide relevant components. This same reasoning also applies to the `max_depth` parameter which will be introduced later on in the cookbook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604f869b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global tokenizer name to use consistently throughout the code\n",
    "TOKENIZER_NAME = \"o200k_base\"\n",
    "\n",
    "def split_into_20_chunks(text: str, min_tokens: int = 500) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Split text into up to 20 chunks, respecting sentence boundaries and ensuring\n",
    "    each chunk has at least min_tokens (unless it's the last chunk).\n",
    "    \n",
    "    Args:\n",
    "        text: The text to split\n",
    "        min_tokens: The minimum number of tokens per chunk (default: 500)\n",
    "    \n",
    "    Returns:\n",
    "        A list of dictionaries where each dictionary has:\n",
    "        - id: The chunk ID (0-19)\n",
    "        - text: The chunk text content\n",
    "    \"\"\"\n",
    "    # First, split the text into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    # Get tokenizer for counting tokens\n",
    "    tokenizer = tiktoken.get_encoding(TOKENIZER_NAME)\n",
    "    \n",
    "    # Create chunks that respect sentence boundaries and minimum token count\n",
    "    chunks = []\n",
    "    current_chunk_sentences = []\n",
    "    current_chunk_tokens = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        # Count tokens in this sentence\n",
    "        sentence_tokens = len(tokenizer.encode(sentence))\n",
    "        \n",
    "        # If adding this sentence would make the chunk too large AND we already have the minimum tokens,\n",
    "        # finalize the current chunk and start a new one\n",
    "        if (current_chunk_tokens + sentence_tokens > min_tokens * 2) and current_chunk_tokens >= min_tokens:\n",
    "            chunk_text = \" \".join(current_chunk_sentences)\n",
    "            chunks.append({\n",
    "                \"id\": len(chunks),  # Integer ID instead of string\n",
    "                \"text\": chunk_text\n",
    "            })\n",
    "            current_chunk_sentences = [sentence]\n",
    "            current_chunk_tokens = sentence_tokens\n",
    "        else:\n",
    "            # Add this sentence to the current chunk\n",
    "            current_chunk_sentences.append(sentence)\n",
    "            current_chunk_tokens += sentence_tokens\n",
    "    \n",
    "    # Add the last chunk if there's anything left\n",
    "    if current_chunk_sentences:\n",
    "        chunk_text = \" \".join(current_chunk_sentences)\n",
    "        chunks.append({\n",
    "            \"id\": len(chunks),  # Integer ID instead of string\n",
    "            \"text\": chunk_text\n",
    "        })\n",
    "    \n",
    "    # If we have more than 20 chunks, consolidate them\n",
    "    if len(chunks) > 20:\n",
    "        # Recombine all text\n",
    "        all_text = \" \".join(chunk[\"text\"] for chunk in chunks)\n",
    "        # Re-split into exactly 20 chunks, without minimum token requirement\n",
    "        sentences = sent_tokenize(all_text)\n",
    "        sentences_per_chunk = len(sentences) // 20 + (1 if len(sentences) % 20 > 0 else 0)\n",
    "        \n",
    "        chunks = []\n",
    "        for i in range(0, len(sentences), sentences_per_chunk):\n",
    "            # Get the sentences for this chunk\n",
    "            chunk_sentences = sentences[i:i+sentences_per_chunk]\n",
    "            # Join the sentences into a single text\n",
    "            chunk_text = \" \".join(chunk_sentences)\n",
    "            # Create a chunk object with ID and text\n",
    "            chunks.append({\n",
    "                \"id\": len(chunks),  # Integer ID instead of string\n",
    "                \"text\": chunk_text\n",
    "            })\n",
    "    \n",
    "    # Print chunk statistics\n",
    "    print(f\"Split document into {len(chunks)} chunks\")\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        token_count = len(tokenizer.encode(chunk[\"text\"]))\n",
    "        print(f\"Chunk {i}: {token_count} tokens\")\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Split the document into 20 chunks with minimum token size\n",
    "document_chunks = split_into_20_chunks(document_text, min_tokens=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccc89e6",
   "metadata": {},
   "source": [
    "### 3.3 Router Function with Improved Tool Schema\n",
    "\n",
    "Now, let's create the router function that will select relevant chunks and maintain a scratchpad.\n",
    "\n",
    "> Maintaining a scratchpad allows the model to track decision criteria and reasoning over time. This implementation uses a two-pass approach with GPT-4.1-mini: first requiring the model to update the scratchpad via a tool call (tool_choice=\"required\"), then requesting structured JSON output for chunk selection. This approach provides better visibility into the model's reasoning process while ensuring consistent structured outputs for downstream processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8373af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI()\n",
    "\n",
    "def route_chunks(question: str, chunks: List[Dict[str, Any]], \n",
    "                depth: int, scratchpad: str = \"\") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Ask the model which chunks contain information relevant to the question.\n",
    "    Maintains a scratchpad for the model's reasoning.\n",
    "    Uses structured output for chunk selection and required tool calls for scratchpad.\n",
    "    \n",
    "    Args:\n",
    "        question: The user's question\n",
    "        chunks: List of chunks to evaluate\n",
    "        depth: Current depth in the navigation hierarchy\n",
    "        scratchpad: Current scratchpad content\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with selected IDs and updated scratchpad\n",
    "    \"\"\"\n",
    "    print(f\"\\n==== ROUTING AT DEPTH {depth} ====\")\n",
    "    print(f\"Evaluating {len(chunks)} chunks for relevance\")\n",
    "    \n",
    "    # Build system message\n",
    "    system_message = \"\"\"You are an expert document navigator. Your task is to:\n",
    "1. Identify which text chunks might contain information to answer the user's question\n",
    "2. Record your reasoning in a scratchpad for later reference\n",
    "3. Choose chunks that are most likely relevant. Be selective, but thorough. Choose as many chunks as you need to answer the question, but avoid selecting too many.\n",
    "\n",
    "First think carefully about what information would help answer the question, then evaluate each chunk.\n",
    "\"\"\"\n",
    "\n",
    "    # Build user message with chunks and current scratchpad\n",
    "    user_message = f\"QUESTION: {question}\\n\\n\"\n",
    "    \n",
    "    if scratchpad:\n",
    "        user_message += f\"CURRENT SCRATCHPAD:\\n{scratchpad}\\n\\n\"\n",
    "    \n",
    "    user_message += \"TEXT CHUNKS:\\n\\n\"\n",
    "    \n",
    "    # Add each chunk to the message\n",
    "    for chunk in chunks:\n",
    "        user_message += f\"CHUNK {chunk['id']}:\\n{chunk['text']}\\n\\n\"\n",
    "    \n",
    "    # Define function schema for scratchpad tool calling\n",
    "    tools = [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"name\": \"update_scratchpad\",\n",
    "            \"description\": \"Record your reasoning about why certain chunks were selected\",\n",
    "            \"strict\": True,\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"text\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"Your reasoning about the chunk(s) selection\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"text\"],\n",
    "                \"additionalProperties\": False\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Define JSON schema for structured output (selected chunks)\n",
    "    text_format = {\n",
    "        \"format\": {\n",
    "            \"type\": \"json_schema\",\n",
    "            \"name\": \"selected_chunks\",\n",
    "            \"strict\": True,\n",
    "            \"schema\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"chunk_ids\": {\n",
    "                        \"type\": \"array\",\n",
    "                        \"items\": {\"type\": \"integer\"},\n",
    "                        \"description\": \"IDs of the selected chunks that contain information to answer the question\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\n",
    "                    \"chunk_ids\"\n",
    "                ],\n",
    "                \"additionalProperties\": False\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # First pass: Call the model to update scratchpad (required tool call)\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": user_message + \"\\n\\nFirst, you must use the update_scratchpad function to record your reasoning.\"}\n",
    "    ]\n",
    "    \n",
    "    response = client.responses.create(\n",
    "        model=\"gpt-4.1-mini\",\n",
    "        input=messages,\n",
    "        tools=tools,\n",
    "        tool_choice=\"required\"\n",
    "    )\n",
    "    \n",
    "    # Process the scratchpad tool call\n",
    "    new_scratchpad = scratchpad\n",
    "    \n",
    "    for tool_call in response.output:\n",
    "        if tool_call.type == \"function_call\" and tool_call.name == \"update_scratchpad\":\n",
    "            args = json.loads(tool_call.arguments)\n",
    "            scratchpad_entry = f\"DEPTH {depth} REASONING:\\n{args.get('text', '')}\"\n",
    "            if new_scratchpad:\n",
    "                new_scratchpad += \"\\n\\n\" + scratchpad_entry\n",
    "            else:\n",
    "                new_scratchpad = scratchpad_entry\n",
    "            \n",
    "            # Add function call and result to messages\n",
    "            messages.append(tool_call)\n",
    "            messages.append({\n",
    "                \"type\": \"function_call_output\",\n",
    "                \"call_id\": tool_call.call_id,\n",
    "                \"output\": \"Scratchpad updated successfully.\"\n",
    "            })\n",
    "    \n",
    "    # Second pass: Get structured output for chunk selection\n",
    "    messages.append({\"role\": \"user\", \"content\": \"Now, select the chunks that could contain information to answer the question. Return a JSON object with the list of chunk IDs.\"})\n",
    "    \n",
    "    response_chunks = client.responses.create(\n",
    "        model=\"gpt-4.1-mini\",\n",
    "        input=messages,\n",
    "        text=text_format\n",
    "    )\n",
    "    \n",
    "    # Extract selected chunk IDs from structured output\n",
    "    selected_ids = []\n",
    "    if response_chunks.output_text:\n",
    "        try:\n",
    "            # The output_text should already be in JSON format due to the schema\n",
    "            chunk_data = json.loads(response_chunks.output_text)\n",
    "            selected_ids = chunk_data.get(\"chunk_ids\", [])\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"Warning: Could not parse structured output as JSON\")\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"Selected chunks: {', '.join(str(id) for id in selected_ids)}\")\n",
    "    print(f\"Updated scratchpad:\\n{new_scratchpad}\")\n",
    "    \n",
    "    return {\n",
    "        \"selected_ids\": selected_ids,\n",
    "        \"scratchpad\": new_scratchpad\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11654a9",
   "metadata": {},
   "source": [
    "### 3.4 Recursive Navigation Function\n",
    "\n",
    "Now, let's create the recursive navigation function that drills down through the document. `max_depth` is the maximum number of levels to drill down (keeping token minimums in mind):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "876940b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def navigate_to_paragraphs(document_text: str, question: str, max_depth: int = 1) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Navigate through the document hierarchy to find relevant paragraphs.\n",
    "    \n",
    "    Args:\n",
    "        document_text: The full document text\n",
    "        question: The user's question\n",
    "        max_depth: Maximum depth to navigate before returning paragraphs (default: 1)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with selected paragraphs and final scratchpad\n",
    "    \"\"\"\n",
    "    scratchpad = \"\"\n",
    "    \n",
    "    # Get initial chunks with min 500 tokens\n",
    "    chunks = split_into_20_chunks(document_text, min_tokens=500)\n",
    "    \n",
    "    # Navigator state - track chunk paths to maintain hierarchy\n",
    "    chunk_paths = {}  # Maps numeric IDs to path strings for display\n",
    "    for chunk in chunks:\n",
    "        chunk_paths[chunk[\"id\"]] = str(chunk[\"id\"])\n",
    "    \n",
    "    # Navigate through levels until max_depth or until no chunks remain\n",
    "    for current_depth in range(max_depth + 1):\n",
    "        # Call router to get relevant chunks\n",
    "        result = route_chunks(question, chunks, current_depth, scratchpad)\n",
    "        \n",
    "        # Update scratchpad\n",
    "        scratchpad = result[\"scratchpad\"]\n",
    "        \n",
    "        # Get selected chunks\n",
    "        selected_ids = result[\"selected_ids\"]\n",
    "        selected_chunks = [c for c in chunks if c[\"id\"] in selected_ids]\n",
    "        \n",
    "        # If no chunks were selected, return empty result\n",
    "        if not selected_chunks:\n",
    "            print(\"\\nNo relevant chunks found.\")\n",
    "            return {\"paragraphs\": [], \"scratchpad\": scratchpad}\n",
    "        \n",
    "        # If we've reached max_depth, return the selected chunks\n",
    "        if current_depth == max_depth:\n",
    "            print(f\"\\nReturning {len(selected_chunks)} relevant chunks at depth {current_depth}\")\n",
    "            \n",
    "            # Update display IDs to show hierarchy\n",
    "            for chunk in selected_chunks:\n",
    "                chunk[\"display_id\"] = chunk_paths[chunk[\"id\"]]\n",
    "                \n",
    "            return {\"paragraphs\": selected_chunks, \"scratchpad\": scratchpad}\n",
    "        \n",
    "        # Prepare next level by splitting selected chunks further\n",
    "        next_level_chunks = []\n",
    "        next_chunk_id = 0  # Counter for new chunks\n",
    "        \n",
    "        for chunk in selected_chunks:\n",
    "            # Split this chunk into smaller pieces\n",
    "            sub_chunks = split_into_20_chunks(chunk[\"text\"], min_tokens=200)\n",
    "            \n",
    "            # Update IDs and maintain path mapping\n",
    "            for sub_chunk in sub_chunks:\n",
    "                path = f\"{chunk_paths[chunk['id']]}.{sub_chunk['id']}\"\n",
    "                sub_chunk[\"id\"] = next_chunk_id\n",
    "                chunk_paths[next_chunk_id] = path\n",
    "                next_level_chunks.append(sub_chunk)\n",
    "                next_chunk_id += 1\n",
    "        \n",
    "        # Update chunks for next iteration\n",
    "        chunks = next_level_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d803dfc",
   "metadata": {},
   "source": [
    "### 3.5 Run the Improved Navigation for a Sample Question\n",
    "\n",
    "Let's run the navigation for a sample question with our improved approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e29008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the navigation for a sample question\n",
    "question = \"What format should a motion to compel discovery be filed in? How should signatures be handled?\"\n",
    "navigation_result = navigate_to_paragraphs(document_text, question, max_depth=2)\n",
    "\n",
    "# Sample retrieved paragraph\n",
    "print(\"\\n==== FIRST 3 RETRIEVED PARAGRAPHS ====\")\n",
    "for i, paragraph in enumerate(navigation_result[\"paragraphs\"][:3]):\n",
    "    display_id = paragraph.get(\"display_id\", str(paragraph[\"id\"]))\n",
    "    print(f\"\\nPARAGRAPH {i+1} (ID: {display_id}):\")\n",
    "    print(\"-\" * 40)\n",
    "    print(paragraph[\"text\"])\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf85b3e",
   "metadata": {},
   "source": [
    "\n",
    "The chunking process mirrors legal analysis methodology with striking precision. At Depth 0, the system begins with broad categorization (\"Chunk 0 includes detailed information on filings, electronic filing via ESTTA, signature requirements\"), then refines to specific procedural sections at Depth 1 (\"Chunks 500-530 cover stipulations and motions procedures including motions to compel discovery\"), ultimately focusing on precise rules at Depth 2 (\"Chunks 11-23 provide detailed rules on identification and signature of submissions\"). This progressive refinement from general principles to specific provisions perfectly parallels how attorneys analyze legal questions by first identifying relevant areas of law, then narrowing to specific statutes, and finally focusing on controlling provisions and precedent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495a5230",
   "metadata": {},
   "source": [
    "### 3.6 Answer Generation\n",
    "\n",
    "Now, let's generate an answer using GPT-4.1 with the retrieved paragraphs. \n",
    "\n",
    "> We do a nifty trick here where we dynamically construct a List of Literals (which forces the model's answers to be one of the options we provide -- in this case the paragraph IDs). There are some restrictions on the number of options we can provide, so if you find your system citing > 500 documents, then this solution might not work. In that case, you can either have a filter to go up to 500 potential citations, or you can ask the model to cite the exact ID in it's response, then post-process the response to extract the IDs, thus the citations (e.g. it might say \"... [doc 0.0.12]\", and you could use some regex to extract the citation).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74cfe50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "from pydantic import BaseModel, field_validator\n",
    "\n",
    "class LegalAnswer(BaseModel):\n",
    "    \"\"\"Structured response format for legal questions\"\"\"\n",
    "    answer: str\n",
    "    citations: List[str]\n",
    "    \n",
    "    @field_validator('citations')\n",
    "    def validate_citations(cls, citations, info):\n",
    "        # Access valid_citations from the model_config\n",
    "        valid_citations = info.data.get('_valid_citations', [])\n",
    "        if valid_citations:\n",
    "            for citation in citations:\n",
    "                if citation not in valid_citations:\n",
    "                    raise ValueError(f\"Invalid citation: {citation}. Must be one of: {valid_citations}\")\n",
    "        return citations\n",
    "\n",
    "def generate_answer(question: str, paragraphs: List[Dict[str, Any]], \n",
    "                   scratchpad: str) -> LegalAnswer:\n",
    "    \"\"\"Generate an answer from the retrieved paragraphs.\"\"\"\n",
    "    print(\"\\n==== GENERATING ANSWER ====\")\n",
    "    \n",
    "    # Extract valid citation IDs\n",
    "    valid_citations = [str(p.get(\"display_id\", str(p[\"id\"]))) for p in paragraphs]\n",
    "    \n",
    "    if not paragraphs:\n",
    "        return LegalAnswer(\n",
    "            answer=\"I couldn't find relevant information to answer this question in the document.\",\n",
    "            citations=[],\n",
    "            _valid_citations=[]\n",
    "        )\n",
    "    \n",
    "    # Prepare context for the model\n",
    "    context = \"\"\n",
    "    for paragraph in paragraphs:\n",
    "        display_id = paragraph.get(\"display_id\", str(paragraph[\"id\"]))\n",
    "        context += f\"PARAGRAPH {display_id}:\\n{paragraph['text']}\\n\\n\"\n",
    "    \n",
    "    system_prompt = \"\"\"You are a legal research assistant answering questions about the \n",
    "Trademark Trial and Appeal Board Manual of Procedure (TBMP).\n",
    "\n",
    "Answer questions based ONLY on the provided paragraphs. Do not rely on any foundation knowledge or external information or extrapolate from the paragraphs.\n",
    "Cite phrases of the paragraphs that are relevant to the answer. This will help you be more specific and accurate.\n",
    "Include citations to paragraph IDs for every statement in your answer. Valid citation IDs are: {valid_citations_str}\n",
    "Keep your answer clear, precise, and professional.\n",
    "\"\"\"\n",
    "    valid_citations_str = \", \".join(valid_citations)\n",
    "    \n",
    "    # Call the model using structured output\n",
    "    response = client.responses.parse(\n",
    "        model=\"gpt-4.1\",\n",
    "        input=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt.format(valid_citations_str=valid_citations_str)},\n",
    "            {\"role\": \"user\", \"content\": f\"QUESTION: {question}\\n\\nSCRATCHPAD (Navigation reasoning):\\n{scratchpad}\\n\\nPARAGRAPHS:\\n{context}\"}\n",
    "        ],\n",
    "        text_format=LegalAnswer,\n",
    "        temperature=0.3\n",
    "    )\n",
    "    \n",
    "    # Add validation information after parsing\n",
    "    response.output_parsed._valid_citations = valid_citations\n",
    "    \n",
    "    print(f\"\\nAnswer: {response.output_parsed.answer}\")\n",
    "    print(f\"Citations: {response.output_parsed.citations}\")\n",
    "\n",
    "    return response.output_parsed\n",
    "\n",
    "# Generate an answer\n",
    "answer = generate_answer(question, navigation_result[\"paragraphs\"], \n",
    "                       navigation_result[\"scratchpad\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d5e682",
   "metadata": {},
   "source": [
    "GPT-4.1 then assembles comprehensive legal responses by referencing specific chunks (through paragraph IDs like \"0.0.0.6.4\") while weaving in critical procedural specifications—including the \"/John Doe/\" electronic signature format and mandatory 11-point, double-spaced typography requirements—extracted verbatim from authoritative TTAB documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9cfe43b",
   "metadata": {},
   "source": [
    "### 3.7 Answer Verification\n",
    "\n",
    "Let's first look at the cited paragraphs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5e9cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cited_paragraphs = []\n",
    "for paragraph in navigation_result[\"paragraphs\"]:\n",
    "    para_id = str(paragraph.get(\"display_id\", str(paragraph[\"id\"])))\n",
    "    if para_id in answer.citations:\n",
    "        cited_paragraphs.append(paragraph)\n",
    "    \n",
    "\n",
    "# Display the cited paragraphs for the audience\n",
    "print(\"\\n==== CITED PARAGRAPHS ====\")\n",
    "for i, paragraph in enumerate(cited_paragraphs):\n",
    "    display_id = paragraph.get(\"display_id\", str(paragraph[\"id\"]))\n",
    "    print(f\"\\nPARAGRAPH {i+1} (ID: {display_id}):\")\n",
    "    print(\"-\" * 40)\n",
    "    print(paragraph[\"text\"])\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36a8431",
   "metadata": {},
   "source": [
    "The \"List of Literals\" trick forces the model to cite only specific paragraph IDs (like \"0.0.0.5.4\") rather than making up its own references or highlighting random text — imagine it as creating a digital \"table of contents\" that GPT-4.1 can only select from. This solution ensures you get verifiable citation trails back to exact source material, solving an important problem in long-context RAG."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b1eb2d",
   "metadata": {},
   "source": [
    "Finally, let's verify the answer with an LLM-as-judge approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a765a9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any, Literal\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class VerificationResult(BaseModel):\n",
    "    \"\"\"Verification result format\"\"\"\n",
    "    is_accurate: bool\n",
    "    explanation: str\n",
    "    confidence: Literal[\"high\", \"medium\", \"low\"]\n",
    "\n",
    "def verify_answer(question: str, answer: LegalAnswer, \n",
    "                 cited_paragraphs: List[Dict[str, Any]]) -> VerificationResult:\n",
    "    \"\"\"\n",
    "    Verify if the answer is grounded in the cited paragraphs.\n",
    "    \n",
    "    Args:\n",
    "        question: The user's question\n",
    "        answer: The generated answer\n",
    "        cited_paragraphs: Paragraphs cited in the answer\n",
    "        \n",
    "    Returns:\n",
    "        Verification result with accuracy assessment, explanation, and confidence level\n",
    "    \"\"\"\n",
    "    print(\"\\n==== VERIFYING ANSWER ====\")\n",
    "    \n",
    "    # Prepare context with the cited paragraphs\n",
    "    context = \"\"\n",
    "    for paragraph in cited_paragraphs:\n",
    "        display_id = paragraph.get(\"display_id\", str(paragraph[\"id\"]))\n",
    "        context += f\"PARAGRAPH {display_id}:\\n{paragraph['text']}\\n\\n\"\n",
    "    \n",
    "    # Prepare system prompt\n",
    "    system_prompt = \"\"\"You are a fact-checker for legal information.\n",
    "Your job is to verify if the provided answer:\n",
    "1. Is factually accurate according to the source paragraphs\n",
    "2. Uses citations correctly\n",
    "\n",
    "Be critical and look for any factual errors or unsupported claims.\n",
    "Assign a confidence level based on how directly the paragraphs answer the question:\n",
    "- high: The answer is comprehensive, accurate, and directly supported by the paragraphs\n",
    "- medium: The answer is mostly accurate but may be incomplete or have minor issues\n",
    "- low: The answer has significant gaps, inaccuracies, or is poorly supported by the paragraphs\n",
    "\"\"\"\n",
    "    \n",
    "    response = client.responses.parse(\n",
    "        model=\"o4-mini\",\n",
    "        input=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": f\"\"\"\n",
    "QUESTION: {question}\n",
    "\n",
    "ANSWER TO VERIFY:\n",
    "{answer.answer}\n",
    "\n",
    "CITATIONS USED: {', '.join(answer.citations)}\n",
    "\n",
    "SOURCE PARAGRAPHS:\n",
    "{context}\n",
    "\n",
    "Is this answer accurate and properly supported by the source paragraphs?\n",
    "Assign a confidence level (high, medium, or low) based on completeness and accuracy.\n",
    "            \"\"\"}\n",
    "        ],\n",
    "        text_format=VerificationResult\n",
    "    )\n",
    "    \n",
    "    # Log and return the verification result\n",
    "    print(f\"\\nAccuracy verification: {'PASSED' if response.output_parsed.is_accurate else 'FAILED'}\")\n",
    "    print(f\"Confidence: {response.output_parsed.confidence}\")\n",
    "    print(f\"Explanation: {response.output_parsed.explanation}\")\n",
    "    \n",
    "    return response.output_parsed\n",
    "\n",
    "# Verify the answer using only the cited paragraphs\n",
    "verification = verify_answer(question, answer, cited_paragraphs)\n",
    "\n",
    "# Display final result with verification\n",
    "print(\"\\n==== FINAL VERIFIED ANSWER ====\")\n",
    "print(f\"Verification: {'PASSED' if verification.is_accurate else 'FAILED'} | Confidence: {verification.confidence}\")\n",
    "print(\"\\nAnswer:\")\n",
    "print(answer.answer)\n",
    "print(\"\\nCitations:\")\n",
    "for citation in answer.citations:\n",
    "    print(f\"- {citation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1004942a",
   "metadata": {},
   "source": [
    "The verification step produces a clean, structured assessment that references specific regulations and methodically checks both the answer's accuracy and its proper use of citations. Rather than just saying \"correct,\" it offers useful context by explaining exactly why the answer was correct, giving you the confidence to then present the answer to the user with specific citations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bc9113",
   "metadata": {},
   "source": [
    "## 4. Infrastructure Costs\n",
    "\n",
    "Let's break down the cost structure for this agentic RAG approach:\n",
    "\n",
    "### Estimated Fixed vs. Variable Costs\n",
    "\n",
    "* **Estimated Fixed (One-time) Costs:**  \n",
    "  * **Traditional RAG:** ~$0.43 (embedding + metadata generation)\n",
    "  * **Agentic RAG:** $0.00 (zero preprocessing required)\n",
    "\n",
    "\n",
    "* **Estimated Variable (Per-Query) Costs:**  \n",
    "  * **Router Model (`gpt-4.1-mini`):**  \n",
    "    * Initial routing (20 chunks): ~$0.10  \n",
    "    * Two recursive levels: ~$0.20\n",
    "  * **Synthesis (`gpt-4.1`):** ~$0.05\n",
    "  * **Verification (`o4-mini`):** ~$0.01\n",
    "  * **Total per query:** ~$0.36\n",
    "\n",
    "While the per-query cost is higher than traditional RAG, this approach offers:\n",
    "- Immediate results on new documents\n",
    "- More precise citations\n",
    "- Better handling of paraphrases and conceptual questions\n",
    "- No infrastructure maintenance overhead\n",
    "\n",
    "The cost can be optimized through:\n",
    "- Caching results for common queries\n",
    "- Limiting max tokens in the model calls\n",
    "- Using a hybrid approach that pre-filters the document first\n",
    "\n",
    "## 5. Benefits and Tradeoffs versus Traditional RAG\n",
    "\n",
    "### Benefits\n",
    "- **Zero-ingest latency**: Answer questions from new documents immediately, with no preprocessing.\n",
    "- **Dynamic navigation**: Mimics human reading patterns by focusing on promising sections.\n",
    "- **Cross-section reasoning**: Model can find connections across document sections that might be missed by independent chunk retrieval, potentially increasing accuracy of generated answers and saving time on optimizing retrieval pipelines.\n",
    "\n",
    "### Tradeoffs\n",
    "- **Higher per-query cost**: Requires more computation for each question compared to embedding-based retrieval.\n",
    "- **Increased latency**: Hierarchical navigation takes longer to process than simple vector lookups.\n",
    "- **Limited scalability**: May struggle with extremely large document collections where preprocessing becomes more efficient.\n",
    "\n",
    "## 6. Takeaways\n",
    "\n",
    "1. **Context Window is a Superpower:** Million-token context windows make it possible to navigate documents on-the-fly.\n",
    "2. **Hierarchical Approach Mimics Human Reading:** Agentic routing works like a human skimming a document for relevant sections.\n",
    "3. **Scratchpad Enables Multi-Step Reasoning:** Maintaining a reasoning record improves navigation quality.\n",
    "4. **Fast Implementation, No Database:** The entire system can be built with just API calls, no infrastructure needed.\n",
    "5. **Verification Improves Reliability:** The LLM-as-judge pattern catches errors before they reach users.\n",
    "\n",
    "================================================================================\n",
    "\n",
    "## 3B. Use Case: AI Co-Scientist for Pharma R&D\n",
    "![AI Co-Scientist for Pharma R&D](../../../images/3B_reasoning_task_card.png)\n",
    "\n",
    "This section details how to build an AI system that functions as a \"co-scientist\" to accelerate experimental design in pharmaceutical R&D, focusing on optimizing a drug synthesis process under specific constraints.\n",
    "\n",
    "## 🗂️ TL;DR Matrix\n",
    "\n",
    "This table summarizes the core technology choices and their rationale for this specific AI Co-Scientist implementation.\n",
    "\n",
    "| Layer              | Choice                                                                  | Utility                                                                                           |\n",
    "| :----------------- | :------------------------------------------------------------------------ | :------------------------------------------------------------------------------------------------------- |\n",
    "| **Ideation**       | `o4-mini` (Parallel Role-Playing Agents)                                  | Generates diverse hypotheses & protocols rapidly and cost-effectively; role-playing enhances creativity. |\n",
    "| **Grounding**      | External Tool Calls (`chem_lookup`, `cost_estimator`, `outcome_db`, etc.) | Ensures plans are based on real-world data (chemical properties, costs, past results).                   |\n",
    "| **Ranking**        | `o4-mini` (Pairwise Tournament Comparison)                                | Nuanced evaluation beyond simple scoring; selects promising candidates efficiently.                      |\n",
    "| **Critique/Synth** | `o3` (Deep Review & Synthesis)                                            | Provides rigorous, senior-level analysis, identifies risks, and ensures scientific validity.             |\n",
    "| **Safety (Opt.)**  | `gpt-4.1-mini` (Targeted Check)                                       | Adds an extra layer of specialized safety review before human handoff.                                   |\n",
    "| **Learning**       | `o3` + Code Interpreter (Result Analysis → DB)                            | Captures experimental outcomes systematically, enabling continuous improvement over time.                |\n",
    "| **Core Technique** | Multi-Agent Collaboration & Escalation                                    | Leverages strengths of different models (speed vs. depth) for a complex, multi-step reasoning task.      |\n",
    "\n",
    "*Note: Model identifiers accurate as of April 2025, subject to change.*\n",
    "\n",
    "## 1. Scenario Snapshot\n",
    "\n",
    "* **Problem Space:** Optimizing complex experimental procedures in pharmaceutical R&D, such as improving the synthesis yield of a new drug compound (\"XYZ-13\") while adhering to strict constraints.\n",
    "* **Users:** Research scientists and lab technicians involved in drug discovery and development.\n",
    "* **Typical Asks:**\n",
    "    1. Suggest 3 distinct protocols to increase XYZ-13 yield by ≥15% by testing different catalysts, staying under $15k using approved reagents.\n",
    "    2. Propose protocols to optimize XYZ-13 yield below 60°C (due to past heat issues), exploring different approved solvents within budget.\n",
    "    3. Design two XYZ-13 yield strategies (aiming for ≥15%): a. one maximizing potential yield within the \\$15k budget, b. one prioritizing cost under \\$10k.\n",
    "* **Constraints:**\n",
    "    * **Budgetary:** Operate within defined financial limits (e.g., $15,000 per experiment series).\n",
    "    * **Regulatory/Safety:** Use only pre-approved chemicals/reagents and adhere rigorously to safety protocols.\n",
    "    * **Human Oversight:** Final experimental plans must be reviewed and validated by a human expert before execution.\n",
    "\n",
    "> Traditionally, optimizing such experiments involves weeks of manual planning, literature review, iterative benchwork, and analysis. This AI Co-Scientist approach aims to dramatically reduce the cycle time by automating hypothesis generation, protocol design, and preliminary evaluation, enabling scientists to focus on higher-level strategy and final validation. It shifts the scientist's role from manual execution of planning steps to expert oversight and collaboration with the AI.\n",
    "\n",
    "\n",
    "## 2. Architecture (Multi-Agent Reasoning)\n",
    "\n",
    "The system employs a multi-agent architecture that emulates a high-performing scientific team. Different AI components, acting in specialized roles (such as ideation, critique, and learning from outcomes), collaborate using various models and tools to execute the workflow.\n",
    "\n",
    "![AI Co-Scientist Architecture](../../../images/3B_coscientist_architecture.png)\n",
    "\n",
    "### 2.1. **Scientist Input & Constraints:** \n",
    "The process starts with the scientist defining the goal, target compound, and constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "abbeddb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from agent_utils import Context, call_openai, log_json\n",
    "\n",
    "# Example Initial Input\n",
    "user_input = {\n",
    "    \"compound\": \"XYZ-13\",\n",
    "    \"goal\": \"Improve synthesis yield by 15%\",\n",
    "    \"budget\": 15000,\n",
    "    \"time_h\": 48,\n",
    "    \"previous\": \"Prior attempts failed at high temp; explore potential catalyst effects.\"\n",
    "}\n",
    "ctx = Context(client=OpenAI(), **user_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e791f29f",
   "metadata": {},
   "source": [
    "### 2.2.  **Ideation (`o4-mini` + Tools):** \n",
    "Multiple `o4-mini` instances, prompted with different roles (e.g., `Hypothesis Agent`, `Protocol Agent`, `Resource Agent`), generate experimental plans in parallel. Assigning distinct personas encourages diverse perspectives and covers different aspects of the problem simultaneously during the ideation phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f06fe8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROLE_FOCUS = {\n",
    "    # Hypothesis Agent Prompt\n",
    "    \"hypothesis_agent\": \"\"\"You are a pharmaceutical hypothesis specialist. \n",
    "        Focus exclusively on analyzing the compound structure and research goals to generate testable hypotheses. \n",
    "        Consider mechanism of action, binding affinity predictions, and potential off-target effects.\"\"\",\n",
    "\n",
    "    # Protocol Agent Prompt\n",
    "    \"protocol_agent\"  : \"\"\"You are a laboratory protocol specialist. \n",
    "        Design experimental procedures that will effectively test the provided hypothesis. \n",
    "        Focus on experimental conditions, controls, and measurement techniques.\"\"\",\n",
    "\n",
    "    # Resource Agent Prompt\n",
    "    \"resource_agent\"  : \"\"\"You are a laboratory resource optimization specialist. \n",
    "        Review the proposed protocol and optimize for efficiency. \n",
    "        Identify opportunities to reduce reagent use, equipment time, and overall costs while maintaining scientific validity.\"\"\",\n",
    "}\n",
    "\n",
    "# Create a structured prompt template for ideation\n",
    "IDEATION_PROMPT = \"\"\"You are a pharmaceutical {role} specialist. Your goal is to {goal} for compound {compound}.\n",
    "Constraints:\n",
    "- Budget: ${budget}\n",
    "- Approved reagents only\n",
    "- Complete within {time_h} hours\n",
    "- Previous attempts: {previous}\n",
    "Respond with structured JSON describing your protocol.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf9f5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, logging\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any, Optional\n",
    "from dataclasses import asdict\n",
    "from functools import partial\n",
    "\n",
    "MODEL_IDEATE   = \"o4-mini-2025-04-16\"  # o4-mini model for ideation - balances speed and quality\n",
    "\n",
    "# Configure logging to help with tracking experiment progress and debugging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(message)s\")\n",
    "logging.info(f\"Run‑id {ctx.run_id}  Compound: {ctx.compound}\")\n",
    "logging.info(f\"Logs will be stored in: {Path('logs') / ctx.run_id}\")\n",
    "\n",
    "def ideation(ctx: Context):\n",
    "    logging.info(\"Starting ideation phase...\")\n",
    "    ideas = []\n",
    "    for role, focus in ROLE_FOCUS.items():\n",
    "        logging.info(f\"Running ideation agent ${role}\")\n",
    "        sys = IDEATION_PROMPT.format(role=role, focus=focus, **ctx.prompt_vars())\n",
    "        usr = f\"Design a protocol to {ctx.goal} within ${ctx.budget}.\"\n",
    "        idea = call_openai(ctx.client, MODEL_IDEATE, sys, usr, ctx)\n",
    "        ideas.append(idea)\n",
    "    log_json(\"ideation_done\", ideas, ctx)\n",
    "    return ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0384e0d5",
   "metadata": {},
   "source": [
    "The ideation agents can utilize external tools such as `literature_search`, `chem_lookup` (chemical database), `cost_estimator`, `outcome_db` (outcome of previous experiments) to ground their suggestions in data. Explicitly enabling and prompting models to use external tools ensures that generated plans are feasible, compliant, and informed by existing knowledge. The model decides when and which tool to call based on the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f365d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "IDEATION_PROMPT += \"\"\"\\nUse the following tools as appropriate:\n",
    "- Use the `list_available_chemicals` tool to get list of approved reagents.\n",
    "- Use the `chem_lookup` tool to verify properties of reagents mentioned.\n",
    "- Use the `cost_estimator` tool to calculate the approximate cost based on reagents and proposed steps.\n",
    "- Check the `outcome_db` for relevant prior experiments with {compound}\"\"\"\n",
    "\n",
    "ideas = ideation(ctx)\n",
    "logging.info(\"Ideation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f507348",
   "metadata": {},
   "source": [
    "These tools are defined in `agent_utils.py`. For purposes of this solution, the tool calls are mocked in `tools.py`. In a real use case, these tools would call real APIs.\n",
    "\n",
    "\n",
    "### 2.3. **Tournament Ranking (`o4-mini` / `o3`):** \n",
    "Generated protocols are compared pairwise based on criteria like expected effectiveness, feasibility, cost, and novelty. Instead of asking a model to score protocols in isolation, providing two protocols at a time and asking for a direct comparison against specific criteria often yields more reliable relative rankings.\n",
    "\n",
    "This Elo-style ranking identifies the most promising candidates for deeper review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85fe4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOURNAMENT_PROMPT = \"\"\"\n",
    "Protocol A: [details...]\n",
    "Protocol B: [details...]\n",
    "\n",
    "Compare Protocol A and Protocol B for synthesizing {compound} aimed at {goal}. Score them on:\n",
    "1. Likelihood of achieving ≥ 15% yield increase.\n",
    "2. Practical feasibility (reagents, time).\n",
    "3. Estimated cost-efficiency (use tool if needed).\n",
    "4. Scientific novelty/risk.\n",
    "\n",
    "Return JSON {{\\\"winner\\\": \\\"A\\\"|\\\"B\\\", \\\"justification\\\": \\\"...\\\"}}.\"\"\"\n",
    "\n",
    "# This is a mock tourname implementation that only compares the first two protocols\n",
    "# A real implementation would compare pairs in a tournament bracket style\n",
    "def tournament(protocols: List[Dict[str, Any]], ctx: Context):\n",
    "    logging.info(\"Starting tournament phase...\")\n",
    "    if len(protocols) == 1:\n",
    "        return protocols[:1]\n",
    "    a, b = protocols[0], protocols[1]\n",
    "    sys = TOURNAMENT_PROMPT.format(**ctx.prompt_vars())\n",
    "    usr = json.dumps({\"A\": a, \"B\": b}, indent=2)\n",
    "    res = call_openai(ctx.client, MODEL_IDEATE, sys, usr, ctx)\n",
    "    winner = a if res.get(\"winner\", \"A\").upper() == \"A\" else b\n",
    "    log_json(\"tournament\", res, ctx)\n",
    "    return [winner]\n",
    "\n",
    "top_proto = tournament(ideas, ctx)[0]\n",
    "logging.info(\"Tournament winner picked!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ad4731",
   "metadata": {},
   "source": [
    "> In early experiments, we found that asking models to score protocols on a 1-10 scale led to inconsistent results with score compression. The tournament approach solved this by forcing relative judgments that proved more reliable. This mirrors human expert behavior — scientists often find it easier to compare two options directly than to assign absolute scores.\n",
    "\n",
    "### 2.4. **Deep Critique & Synthesis (`o3`):** \n",
    "The top-ranked protocols are passed to `o3` for rigorous review. `o3` acts like a senior scientist, assessing scientific validity, methodology, safety, budget compliance, and suggesting improvements or synthesizing a final, refined protocol. It may also call tools for verification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634ef4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep critique phase using a more powerful model for rigorous review\n",
    "CRITIQUE_PROMPT = \"\"\"You are a senior researcher reviewing a proposed synthesis protocol \n",
    "for {compound} aiming for {goal}, budget ${budget} using approved reagents. Review the protocol below rigorously:\n",
    "1. Identify scientific flaws or methodological weaknesses.\n",
    "2. Assess safety risks and budget compliance (use `cost_estimator` tool if needed).\n",
    "3. Check for consistency with prior `outcome_db` results if relevant.\n",
    "4. Suggest concrete improvements or rewrite sections if necessary.\n",
    "5. Provide a final go/no-go recommendation.\n",
    "\n",
    "Return JSON {{\\\"revised_protocol\\\": ..., \\\"critique\\\": \\\"...\\\", \\\"recommendation\\\": \\\"go|no-go\\\"}}.\n",
    "\n",
    "Protocol to Review:\n",
    "[Protocol details...]\n",
    "\"\"\"\n",
    "\n",
    "MODEL_CRITIQUE = \"o3-2025-04-16\"  # o3 model for deep critique\n",
    "\n",
    "def critique(protocol: Dict[str, Any], ctx: Context):\n",
    "    logging.info(\"Starting critique phase...\")\n",
    "    sys = CRITIQUE_PROMPT.format(**ctx.prompt_vars())\n",
    "    usr = json.dumps(protocol, indent=2)\n",
    "    crit = call_openai(ctx.client, MODEL_CRITIQUE, sys, usr, ctx)\n",
    "    log_json(\"critique\", crit, ctx)\n",
    "    return crit.get(\"revised_protocol\", protocol)\n",
    "\n",
    "critiqued = critique(top_proto, ctx)\n",
    "logging.info(\"Deep critique completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbd87a7",
   "metadata": {},
   "source": [
    "> We deliberately separate ideation from critique using different models and personas. Having the same model both generate and critique its own work often leads to self-justification rather than objective assessment. The o3 model, acting as a \"senior scientist,\" consistently identified methodological weaknesses that o4-mini missed during ideation.\n",
    "\n",
    "### 2.5. **(Optional) Safety Check:** \n",
    "A specialized model, such as `gpt-4.1-mini`, can perform a final check for specific safety concerns (e.g., hazardous reagent combos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4405e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional safety check using a targeted model\n",
    "SAFETY_PROMPT = \"\"\"You are a lab‑safety specialist. \n",
    "Identify hazards, unsafe conditions, or compliance issues in this protocol for {compound}. \n",
    "Use `chem_lookup` tool if needed. Return JSON assessment.\"\"\"\n",
    "\n",
    "MODEL_SAFETY   = \"gpt-4.1-mini-2025-04-14\"  # gpt-4.1-mini model for safety checks - optimized for instruction following\n",
    "\n",
    "def safety(protocol: Dict[str, Any], ctx: Context):\n",
    "    logging.info(\"Starting safety assessment...\")\n",
    "    sys = SAFETY_PROMPT.format(**ctx.prompt_vars())\n",
    "    usr = json.dumps(protocol, indent=2)\n",
    "    assessment = call_openai(ctx.client, MODEL_SAFETY, sys, usr, ctx)\n",
    "    log_json(\"safety\", assessment, ctx)\n",
    "    return {\"protocol\": protocol, \"safety\": assessment}\n",
    "\n",
    "secured = safety(critiqued, ctx)\n",
    "logging.info(\"Safety check completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd93396",
   "metadata": {},
   "source": [
    "### 2.6. **Human Review:** \n",
    "The AI-generated final plan is presented to the human scientist via an interface for validation, potential edits, and final approval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d47339",
   "metadata": {},
   "outputs": [],
   "source": [
    "def human_review(safety_package: Dict[str, Any], ctx: Context):\n",
    "    logging.info(\"Awaiting human review...\")\n",
    "    protocol = safety_package[\"protocol\"]\n",
    "    safety_assessment = safety_package[\"safety\"]\n",
    "    \n",
    "    print(f\"\\n=== PROTOCOL FOR REVIEW: {ctx.compound} - {ctx.goal} ===\")\n",
    "    print(f\"DETAILS: {json.dumps(protocol, indent=2)}\")\n",
    "    print(f\"SAFETY: {json.dumps(safety_assessment, indent=2)}\")\n",
    "    \n",
    "    while True:\n",
    "        approval = input(\"\\nApprove for execution? (yes/no): \").lower()\n",
    "        if approval in ['yes', 'y', 'no', 'n']:\n",
    "            approved = approval in ['yes', 'y']\n",
    "            logging.info(f\"Protocol {'approved' if approved else 'rejected'}\")\n",
    "            return {\"protocol\": protocol, \"approved\": approved}\n",
    "        print(\"Please enter 'yes' or 'no'\")\n",
    "\n",
    "human_decision = human_review(secured, ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51e598b",
   "metadata": {},
   "source": [
    "### 2.7. **Execution & Learning (`o3` + Code Interpreter):** \n",
    "Once the human approves, the plan is sent for lab execution. After lab execution, results are fed back into the system. `o3` combined with the `Code Interpreter` analyzes the data, generates insights, and stores structured outcomes (protocol, parameters, results, insights) in a database (`Outcome DB`). This database informs future ideation cycles, creating a learning loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3894d1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulating execution and analyzing results\n",
    "ANALYSIS_PROMPT = \"\"\"You are a data analyst. \n",
    "Did the experiment achieve {goal}?  Analyse factors, suggest improvements, and return structured JSON.\n",
    "\"\"\"\n",
    "\n",
    "def execute_and_analyse(pkt: Dict[str, Any], ctx: Context):\n",
    "    logging.info(\"Starting mock execution and analysis...\")\n",
    "    # These are mock results for a lab experiment\n",
    "    mock_results = {\n",
    "        \"yield_improvement\": 12.5,\n",
    "        \"success\": False,\n",
    "        \"actual_cost\": ctx.budget * 0.85,\n",
    "        \"notes\": \"Mock execution\"\n",
    "    }\n",
    "    sys = ANALYSIS_PROMPT.format(**ctx.prompt_vars())\n",
    "    usr = json.dumps({\"protocol\": pkt, \"results\": mock_results}, indent=2)\n",
    "    analysis = call_openai(ctx.client, MODEL_CRITIQUE, sys, usr, ctx)\n",
    "    log_json(\"analysis\", analysis, ctx)\n",
    "    return analysis\n",
    "\n",
    "# Only proceed to execution if approved by the human reviewer\n",
    "if human_decision[\"approved\"]:\n",
    "    summary = execute_and_analyse(human_decision, ctx)\n",
    "    logging.info(\"Analysis complete\")\n",
    "else:\n",
    "    logging.info(\"Protocol rejected by human reviewer - execution skipped\")\n",
    "    summary = None\n",
    "\n",
    "Path(\"output\").mkdir(exist_ok=True)\n",
    "out_path = Path(\"output\") / f\"{ctx.run_id}_summary.json\"\n",
    "out_path.write_text(json.dumps(summary, indent=2))\n",
    "print(f\"\\n🎉 Completed. Summary written to {out_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4ecb9f",
   "metadata": {},
   "source": [
    "## 3. Model Playbook\n",
    "\n",
    "Choosing between `o4-mini` and `o3` depends on the task's complexity and required depth. For other tasks, `gpt-4.1-mini` provides balance between cost and performance, with the more powerful `gpt4.1` recommended when greater capability or nuance is needed.\n",
    "\n",
    "| Task               | Start With     | Upgrade When...                                            | Escalate To  | Rationale                                                                                    |\n",
    "| :----------------- | :------------- | :--------------------------------------------------------- | :----------- | :------------------------------------------------------------------------------------------- |\n",
    "| Ideation & Protocol Generation | `o4-mini` | Hypotheses lack depth or creativity needed for complex chemical synthesis. | `o3` | `o4-mini` rapidly generates diverse protocols cost-effectively. `o3` provides deeper scientific reasoning when more nuanced approaches are required. |\n",
    "| Protocol Ranking | `o4-mini` | Comparison requires deeper scientific assessment or multi-factor trade-offs. | `o3` | Tournament-style ranking with `o4-mini` efficiently identifies promising candidates. Escalate when subtle scientific validity needs evaluation. |\n",
    "| Deep Critique & Synthesis | `o3` | N/A - Already using the most capable model for this critical task. | N/A | `o3` excels at rigorous scientific review, identifying methodological flaws, and synthesizing improvements across complex protocols. This task inherently requires deep reasoning. |\n",
    "| Safety Assessment | `gpt-4.1-mini` | Domain-specific hazards require higher accuracy or specialized knowledge. | `gpt-4.1` | `gpt-4.1-mini` offers a good balance of cost and performance for standard safety checks. Escalate to `gpt4.1` when higher accuracy or more nuanced reasoning is needed for complex safety risks. |\n",
    "\n",
    "**Key Insight:**\n",
    "> This use case exemplifies a powerful pattern: using faster, cheaper models (`o4-mini`) for breadth and initial filtering, then escalating to more powerful models (`o3`) for depth, critical review, and synthesis. This layered approach optimizes for both creativity/speed and rigor/accuracy, while managing computational costs effectively. The integration with tools is essential for grounding the AI's reasoning in verifiable, real-world data.\n",
    "\n",
    "## 4. Deployment Notes\n",
    "\n",
    "Transitioning the AI Co-Scientist from prototype to lab use involves careful planning.\n",
    "\n",
    "* **Cost Control:**\n",
    "    * Implement configurable \"modes\" (such as `Fast`, `Standard`, `Thorough`) that adjust the number of `o4-mini` ideation agents, the depth of `o3` critique, or the use of optional checks to balance result quality with cost and latency.\n",
    "    * Track token usage per stage (ideation, ranking, critique) and per tool call for fine-grained cost monitoring.\n",
    "* **Observability:**\n",
    "    * Log inputs, outputs, model choices, tool calls/responses, latencies, and token counts for each step.\n",
    "    * Monitor the performance of the tournament ranking and the impact of `o3` critiques (such as how often plans are significantly altered or rejected).\n",
    "    * Track user interactions: which plans are approved, edited, or rejected by the human scientist.\n",
    "* **Safety & Compliance:**\n",
    "    * Implement multiple safety layers: constraints in prompts, tool-based checks (such as reagent compatibility via `chem_lookup`), optional dedicated model checks (`gpt-4.1-mini`), automated filters (such as for known hazardous combinations), and mandatory human review.\n",
    "    * Ensure tool endpoints (such as internal databases) meet security requirements.\n",
    "* **Rollout Strategy:** \n",
    "    * Begin with retrospective analysis of past experiments, then move to shadow mode (AI suggests plans alongside human planners), followed by limited live use cases with close monitoring before broader adoption.\n",
    "\n",
    "\n",
    "## 5. Takeaways\n",
    "\n",
    "1. **Model pairing creates synergy**: `o4-mini` covers more ground quickly; `o3` brings precision and depth.\n",
    "2. **Tool integration grounds reasoning in reality**: Real-world data such as chemical costs and safety constraints inform decision-making.\n",
    "3. **Human scientists remain central**: The system empowers experts by removing grunt work—not by replacing them.\n",
    "\n",
    "\n",
    "## 6. Useful Cookbooks & Resources\n",
    "\n",
    "Here are select resources that complement the design and implementation of the AI Co-Scientist system:\n",
    "\n",
    "- **[Orchestrating Agents: Routines and Handoffs](https://cookbook.openai.com/examples/orchestrating_agents)** Structuring multi-agent workflows with routines and handoffs, relevant to the ideation→ranking→critique pipeline.\n",
    "\n",
    "- **[GPT-4.1 Prompting Guide](https://cookbook.openai.com/examples/gpt4-1_prompting_guide)** Advanced prompting, tool use, and task decomposition for improved accuracy in critique and safety reviews.\n",
    "\n",
    "- **[Structured Outputs for Multi-Agent Systems](https://cookbook.openai.com/examples/structured_outputs_multi_agent)** Enforcing consistent JSON outputs with schema validation for agent interoperability.\n",
    "\n",
    "- **[Agents - OpenAI API](https://platform.openai.com/docs/guides/agents)**  \n",
    "  Comprehensive guide to building multi-agent systems with OpenAI tools, covering orchestration, tool use, and best practices foundational to this system's architecture.\n",
    "\n",
    "================================================================================\n",
    "\n",
    "\n",
    "\n",
    "## 3C. Use Case: Insurance Claim Processing\n",
    "\n",
    "![](../../../images/3C_insurance_task_card.png)\n",
    "\n",
    "Many businesses are faced with the task of digitizing hand filled forms. In this section, we will demonstrate how OpenAI can be used to digitize and validate a hand filled insurance form. While this is a common problem for insurance, the same techniques can be applied to a variety of other industries and forms, for example tax forms, invoices, and more.\n",
    "\n",
    "## 🗂️ TL;DR Matrix\n",
    "\n",
    "This table summarizes the core technology choices and their rationale for this specific OCR implementation targeting the insurance use case.\n",
    "\n",
    "| Layer | Choice | Utility |\n",
    "| :---- | :---- | :---- |\n",
    "| JSON Output | Structured output with Pydantic | Easy to specify formatting, adheres to schema better than `JSON mode` |\n",
    "| OCR and Vision | `gpt-4.1` | Powerful OCR and vision capabilities, structured output |\n",
    "| Reasoning | `o4-mini` | Affordable but capable reasoning, function calling available |\n",
    "| Form Validation | Custom function calling | Can provide interaction with custom or internal databases |\n",
    "\n",
    "\\*Note: Prices and model identifiers accurate as of April 2025, subject to change.\n",
    "\n",
    "## 1\\. Scenario Snapshot\n",
    "\n",
    "* **Users:** The target users are insurance servicing and ops teams who need to ingest data from handwritten forms.  \n",
    "* **Typical Asks:** Each form will have a different required structure, as well as different fields that need to be extracted.  \n",
    "* **Constraints:**  \n",
    "  * **Accuracy:** High accuracy is required to ensure that the data is correct and complete.  \n",
    "  * **Uncertainty:** The system must handle uncertainty in the data, such as missing data, ambiguous data, and different formats of the same field. In the event that the model cannot resolve the uncertainty, the system requires a mechanism to request human review.  \n",
    "  * **Performance & Cost:** While system latency is not critical, high accuracy is required while keeping costs under control. We will aim for a cost target of $20 or less per 1000 pages processed.\n",
    "\n",
    "## 2\\. Architecture\n",
    "\n",
    "The high level basic architecture of the solution is shown below.\n",
    "\n",
    "![](../../../images/3C_insurance_architecture.png)\n",
    "\n",
    "This task is complex and requires a wide variety of model capabilities, including vision, function calling, reasoning, and structured output. While `o3` is capable of doing all of these at once, we found during experimentation that `o4-mini` alone was not sufficient to achieve the necessary performance. Due to the higher relative costs of `o3`, we instead opted for a two-stage approach.\n",
    "\n",
    "1. Stage one is performed using the vision capabilities of GPT 4.1. This stage is optimized to extract text with maximum accuracy, leaving uncertainty for the reasoning stage and not making any assumptions not visible on the page. By doing OCR in the first stage, we do not require the reasoning model to work directly from an image, which can be challenging given all the other tasks the reasoning model must perform.  \n",
    "     \n",
    "2. Stage two takes advantage of the reasoning abilities of `o4-mini`. We use `o4-mini` to validate the accuracy of the OCR and to extract the data into a structured format. Importantly, we expect o4-mini to act as the secondary quality gate \\-- if the OCR is incomplete at this stage we can use o4-mini to refine and validate the original results.\n",
    "\n",
    "To demonstrate concretely how this works, let's look at a sample image of an insurance form.\n",
    "\n",
    "![](../../../images/3C_insurance_form.png)\n",
    "\n",
    "While the form itself is fairly straightforward, there is missing data and ambiguous information that will be difficult for a traditional OCR system to fill out correctly. First, notice that the zip code and county have been omitted. Second, the email address of the user is ambiguous \\-- it could be `jsmith1@gmail.com` or `jsmithl@gmail.com`. In the following sections, we will walk through how a well-designed solution can handle these ambiguities and return the correct form results.\n",
    "\n",
    "**Environment Setup & Library Code:**\n",
    "\n",
    "To make our example code more clear, we have broken out environment setup (such as `pip install` commands) and library functions into a separate code block. This will make it easier to focus on only the relevant logic in each step of our solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923344db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Python requirements\n",
    "%pip install -qU pydantic \"openai>=1.76.0\"\n",
    "\n",
    "# All imports\n",
    "import os\n",
    "import json\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# Create the OpenAI client\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"sk-dummykey\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7ccd93f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_conversation_loop(\n",
    "    client,\n",
    "    messages,\n",
    "    tools,\n",
    "    tool_handlers,\n",
    "    response_format,\n",
    "    model,\n",
    "):\n",
    "    \"\"\"Run the OpenAI response completion loop, handling function calls via tool_handlers until parsing final response.\"\"\"\n",
    "    summaries = []\n",
    "    while True:\n",
    "        print(\n",
    "            f\"Requesting completion from model '{model}' (messages={len(messages)})\"\n",
    "        )\n",
    "        response = client.responses.parse(\n",
    "            model=model,\n",
    "            input=messages,\n",
    "            tools=tools,\n",
    "            text_format=response_format,\n",
    "            reasoning={\"summary\": \"auto\"},\n",
    "        )\n",
    "        summaries.append(response.output[0].summary)\n",
    "\n",
    "        if not response.output_parsed:\n",
    "            print(\"Assistant requested tool calls, resolving ...\")\n",
    "\n",
    "            reasoning_msg, tool_call = response.output\n",
    "            messages.append(reasoning_msg)\n",
    "            messages.append({\n",
    "                \"id\": tool_call.id,\n",
    "                \"call_id\": tool_call.call_id,\n",
    "                \"type\": tool_call.type,\n",
    "                \"name\": tool_call.name,\n",
    "                \"arguments\": tool_call.arguments,\n",
    "            })\n",
    "\n",
    "            if tool_call.name in tool_handlers:\n",
    "                try:\n",
    "                    args = json.loads(tool_call.arguments)\n",
    "                except Exception as exc:\n",
    "                    print(\n",
    "                        \"Failed to parse %s arguments: %s\", tool_call.name, exc\n",
    "                    )\n",
    "                    args = {}\n",
    "                result = tool_handlers[tool_call.name](**args)\n",
    "                messages.append(\n",
    "                    {\n",
    "                        \"type\": \"function_call_output\",\n",
    "                        \"call_id\": tool_call.call_id,\n",
    "                        \"output\": str(result),\n",
    "                    }\n",
    "                )\n",
    "                print(f\"Tool call {tool_call.name} complete, result: {str(result)}\")\n",
    "            else:\n",
    "                print(\"Unhandled function call: %s\", tool_call.name)\n",
    "\n",
    "        if response.output_parsed is not None:\n",
    "            print(\"Received parsed result from model\")\n",
    "            return response, summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76755e0d",
   "metadata": {},
   "source": [
    "**Flow Explanation: Stage 1**\n",
    "\n",
    "1. **Image:** The image of the form taken from the user's smartphone is passed to the model. OpenAI's models can accept a variety of image formats, but we typically use a PNG format to keep the text crisp and reduce artifacts. For this example, we pass the image to the model from a publically available content URL. In a production environment, you likely would pass the image as a signed URL to an image hosted in your own cloud storage bucket.  \n",
    "     \n",
    "2. **Structured Output Schema:** We define a Pydantic model that sets the structure of the output data. The model includes all of the fields that we need to extract from the form, along with the appropriate types for each field. Our model is broken into several subcomponents, each of which is a Pydantic model itself and referenced by the parent model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "59263ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PersonContact(BaseModel):\n",
    "    name: str\n",
    "    home_phone: str\n",
    "    work_phone: str\n",
    "    cell_phone: str\n",
    "    email: str\n",
    "\n",
    "class Address(BaseModel):\n",
    "    street: str\n",
    "    city: str\n",
    "    state: str\n",
    "    zip: str\n",
    "    county: str\n",
    "\n",
    "class DwellingDetails(BaseModel):\n",
    "    coverage_a_limit: str\n",
    "    compantion_policy_expiration_date: str\n",
    "    occupancy_of_dwelling: str\n",
    "    type_of_policy: str\n",
    "    unrepaired_structural_damage: bool\n",
    "    construction_type: str\n",
    "    roof_type: str\n",
    "    foundation_type: str\n",
    "    has_post_and_pier_or_post_and_beam_foundation: bool\n",
    "    cripple_walls: bool\n",
    "    number_of_stories: str\n",
    "    living_space_over_garage: bool\n",
    "    number_of_chimneys: str\n",
    "    square_footage: str\n",
    "    year_of_construction: str\n",
    "    anchored_to_foundation: bool\n",
    "    water_heater_secured: bool\n",
    "\n",
    "class InsuranceFormData(BaseModel):\n",
    "    applicant: PersonContact\n",
    "    co_applicant: PersonContact\n",
    "    risk_address: Address\n",
    "    mailing_address_if_different_than_risk_address: Address\n",
    "    participating_insurer: str\n",
    "    companion_policy_number: str\n",
    "    dwelling_details: DwellingDetails\n",
    "    effective_date: str\n",
    "    expiration_date: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e746a3",
   "metadata": {},
   "source": [
    "3. **Run OCR:** Using the vision capabilities of GPT-4.1, we run the first stage of our pipeline to extract the text from the document in a structured format. This initial stage aims to achieve high accuracy while passing through uncertainty to the second stage. Our prompt explicitly instructs the model to avoid inferring inputs and instead to fill out the details as exact as possible. For the image input, we set image input detail to `auto` to infer a detail level that's appropriate to the image. We found in our experiments that `auto` worked well, but if you are seeing quality issues in your OCR processing considering using `high`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1537dad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "OCR_PROMPT = \"\"\"You are a helpful assistant who excels at processing insurance forms.\n",
    "\n",
    "You will be given an image of a hand-filled insurance form. Your job is to OCR the data into the given structured format.\n",
    "Fill out the fields as exactly as possible. If a written character could possibly be ambigious (i.e. l or 1, o or 0), include all possiblities in the field separated by \"OR\", especially for email addresses.\n",
    "\"\"\"\n",
    "\n",
    "user_content = [\n",
    "    {\"type\": \"input_text\", \"text\": \"Here is a photo of the form filled out by the user:\"},\n",
    "    {\n",
    "        \"type\": \"input_image\",\n",
    "        \"image_url\": \"https://drive.usercontent.google.com/download?id=1-tZ526AW3mX1qthvgi8spaaxxeqFG5_6\",\n",
    "        \"detail\": \"auto\",\n",
    "    },\n",
    "]\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": OCR_PROMPT},\n",
    "    {\"role\": \"user\", \"content\": user_content},\n",
    "]\n",
    "\n",
    "response = client.responses.parse(\n",
    "    model=\"gpt-4.1-2025-04-14\",\n",
    "    input=messages,\n",
    "    text_format=InsuranceFormData,\n",
    "    # Set temp to 0 for reproducibility\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "s1_json_results = json.dumps(json.loads(response.output_parsed.model_dump_json()), indent=2)\n",
    "print(s1_json_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42296380",
   "metadata": {},
   "source": [
    "Notice that the output is missing several fields. In the next stage of processing we will take advantage of OpenAI's reasoning models to infer the missing fields where possible.\n",
    "\n",
    "**Flow Explanation: Stage 2**\n",
    "\n",
    "1. **Function Definitions:** We define a set of custom functions that the model can use to resolve uncertainty. In this case, we define a function that can validate email addresses by checking if the email exists. This can be used to resolve the ambiguous email address field where the model must choose between multiple possible values. By default, o4-mini supports built-in tools like web search, which in this case it will use to resolve zip codes and incomplete addresses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "72dc150e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [{\n",
    "    \"type\": \"function\",\n",
    "    \"name\": \"validate_email\",\n",
    "    \"description\": \"Check if an email address is valid and exists.\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"email\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The email address to validate.\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\n",
    "            \"email\"\n",
    "        ],\n",
    "        \"additionalProperties\": False\n",
    "    }\n",
    "},\n",
    "{\n",
    "    \"type\": \"function\",\n",
    "    \"name\": \"search_web\",\n",
    "    \"description\": \"Perform a web search.\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"query\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The search query to run through the search engine.\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\n",
    "            \"query\"\n",
    "        ],\n",
    "        \"additionalProperties\": False\n",
    "    }\n",
    "}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a9b808",
   "metadata": {},
   "source": [
    "2. **Prompt:** We provide a prompt to the model explaining that we have extracted text via OCR and requesting that the model perform reasoning and function calling to fill in the missing or ambiguous fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ae8fcf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = \"\"\"You are a helpful assistant who excels at processing insurance forms.\n",
    "\n",
    "You will be given a javascript representation of an OCR'd document. Consider at which fields are ambigious reason about how to fill them in. Fill any missing fields that are possible to infer from existing data, or search the web. If you cannot fill a field, reason about why.\n",
    "\n",
    "Use the tools provided if necessary to clarify the results. If the OCR system has provided two possibilities, do your best to definitely pick which option is correct.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2b77ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": PROMPT},\n",
    "    {\"role\": \"user\", \"content\": s1_json_results},\n",
    "]\n",
    "\n",
    "# For demonstration purposes, we'll hardcode the correct email answer.\n",
    "def email_mock(*args, **kwargs):\n",
    "    if kwargs[\"email\"] == \"jsmithl@gmail.com\":\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# Reasoning models like `o4-mini` will soon support built-in web search, but for now\n",
    "# we demonstrate this capability using a simple mock function.\n",
    "def web_mock(*args, **kwargs):\n",
    "    if \"855 Brannan\" in kwargs[\"query\"]:\n",
    "        return \"855 Brannan St, San Francisco, 94103, San Francisco County\"\n",
    "    \n",
    "    return \"\"\n",
    "    \n",
    "tool_handlers = {\"validate_email\": email_mock, \"search_web\": web_mock}\n",
    "\n",
    "response, summaries = run_conversation_loop(\n",
    "    client=client,\n",
    "    messages=messages,\n",
    "    tools=tools,\n",
    "    tool_handlers=tool_handlers,\n",
    "    response_format=InsuranceFormData,\n",
    "    model=\"o4-mini-2025-04-16\",\n",
    ")\n",
    "\n",
    "print(json.dumps(json.loads(response.output_parsed.model_dump_json()), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3f3115",
   "metadata": {},
   "source": [
    "You can see that the email address has been refined to a single value, the zip code and county have been filled in, and the mailing address has been filled in by using the risk address. The model has also returned the results in a structured format (with appropriate types such as boolean for yes/no questions), which can be easily parsed by a downstream system.\n",
    "\n",
    "To help us understand and debug the model, we can also print the summary chain-of-thought reasoning produced by the model. This can help expose common failure modes, points where the model is unclear, or incorrect upstream details.\n",
    "\n",
    "While developing this solution the chain-of-thought summaries exposed some incorrectly named and typed schema values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1d4fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for summary in summaries:\n",
    "    for response in summary:\n",
    "        print(response.text + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bd52eb",
   "metadata": {},
   "source": [
    "## 3\\. Model and Capabilities Playbook\n",
    "\n",
    "Selecting the right tool for the job is key to getting the best results. In general, it's a good idea to start with the simplest solution that fits your needs and then upgrade if you need more capabilities.\n",
    "\n",
    "| Task | Start With | Upgrade When... | Escalate To | Rationale |\n",
    "| :---- | :---- | :---- | :---- | :---- |\n",
    "| OCR | `gpt-4.1` | Complex forms that are difficult to understand at a glance | `o3` | `gpt-4.1` is fast and cost-effective for most OCR. `o-3` has the ability to reason about form structure. |\n",
    "| Results Refinement | `o4-mini` | Complex logic for inferring details, many function calls required. | `o3` | Better for very long chains of reasoning, especially with both function calls and structured output. |\n",
    "\n",
    "## 4\\. Evaluation Metrics\n",
    "\n",
    "Track key metrics to ensure the system is performing accurately and as expected.\n",
    "\n",
    "### Critical Metrics\n",
    "\n",
    "* **OCR Accuracy:** Per-character and per-word accuracy.  \n",
    "* **Inferred Field Rate:** Portion unfilled entries correctly inferred from either existing data or function calling.  \n",
    "* **Human Intervention Rate:** How often a document contains an UNKNOWN and must be referred to a human.\n",
    "\n",
    "We recommend building a labeled hold-out set of forms and their expected responses. This dataset should be representative of the expected deployment environment, see the [OpenAI evals](https://platform.openai.com/docs/guides/evals) guide for more detailed information on building and evaluating your system.\n",
    "\n",
    "## 5\\. Deployment Notes\n",
    "\n",
    "Moving from prototype to a production-ready system requires attention to operational details (LLMOps).\n",
    "\n",
    "### Cost Breakdown\n",
    "\n",
    "We will assume that for document ingestion, [batch pricing](https://platform.openai.com/docs/guides/batch) is a viable option due to high latency tolerance (i.e. overnight runs are fine).\n",
    "\n",
    "#### **Stage 1: OCR (Optical Character Recognition)**\n",
    "\n",
    "**Model:** `gpt-4.1`\n",
    "\n",
    "| Type | Tokens | Rate (per 1M) | Cost |\n",
    "| :---- | :---- | :---- | :---- |\n",
    "| Input | 2,000 | $1.00 | $0.002 |\n",
    "| Output | 1,500 | $4.00 | $0.006 |\n",
    "| **Total (Stage 1\\)** |  |  | **$8.00** |\n",
    "\n",
    "#### **Stage 2: Reasoning**\n",
    "\n",
    "**Model:** `o4-mini`\n",
    "\n",
    "| Type | Tokens | Rate (per 1M) | Cost |\n",
    "| :---- | :---- | :---- | :---- |\n",
    "| Input | 2,000 | $0.55 | $0.0011 |\n",
    "| Output | 3,000 | $2.20 | $0.0066 |\n",
    "| **Total (Stage 2\\)** |  |  | **$7.70** |\n",
    "\n",
    "#### Grand Total (per 1,000 pages): **$15.70**\n",
    "\n",
    "Compare this cost to a one-stage `o3` deployment. Assuming equal token usage and batch usage, the additional cost of the more powerful reasoning model would come to $70/1000 pages.\n",
    "\n",
    "### Monitoring & Deployment\n",
    "\n",
    "Monitor your system by logging key metrics:\n",
    "\n",
    "* `llm_model_used`, `llm_input_tokens`, `llm_output_tokens`, `llm_latency_ms` per model  \n",
    "* `total_query_latency_ms`, `estimated_query_cost` per model  \n",
    "* `function_calls_per_document`, `num_email_validation_calls`  \n",
    "* `human_review_required`\n",
    "\n",
    "Pin the specific model version identifier (e.g., `o4-mini-2025-04-16`) used in deployment via configuration/environment variables to prevent unexpected behavior from silent model updates.\n",
    "\n",
    "## 6\\. Useful Cookbooks & Resources\n",
    "\n",
    "Refer to these related resources for deeper dives into specific components:\n",
    "\n",
    "* [Structured Output](https://platform.openai.com/docs/guides/structured-outputs)  \n",
    "* [Vision Models](https://platform.openai.com/docs/guides/images)  \n",
    "* [Function Calling](https://platform.openai.com/docs/guides/function-calling)\n",
    "\n",
    "================================================================================\n",
    "\n",
    "\n",
    "<h2 id=\"prototype-to-production\">Prototype to Production</h2>\n",
    "\n",
    "Transitioning a prototype to production requires careful planning and execution. This checklist highlights critical steps, drawing from our flagship use cases, to ensure your deployment is robust, efficient, and meets business goals.\n",
    "\n",
    "## 🗂️ TL;DR Matrix\n",
    "\n",
    "| Checklist Area | Key Focus / Actions | Why it Matters |\n",
    "| :---- | :---- | :---- |\n",
    "| **Define Success Criteria** | • Define measurable KPIs & SLOs (accuracy, cost, latency). • Ensure targets are measurable via logs. | Provides clear targets; proves value. |\n",
    "| **Document Model Rationale** | • Select initial models deliberately based on trade-offs. • Document the \"why\" behind model choices. | Justifies choices; aids future updates. |\n",
    "| **Robust Evaluation & Testing** | • Build automated tests (\"eval suite\") using a golden set. • Focus on factuality, hallucinations, tool errors.  • Test tool reliability & edge cases. | Ensures quality; prevents regressions before release. |\n",
    "| **Observability & Cost** | • Implement essential logging for monitoring & debugging. • Set cost guardrails (token limits, usage modes). | Enables tuning; keeps spending within budget. |\n",
    "| **Safety & Compliance** | • Use safety mechanisms (moderation APIs, prompts). • Enforce domain-specific compliance rules. • Mandate Human-in-the-Loop (HITL) for high-risk outputs. | Ensures responsible operation; meets requirements. |\n",
    "| **Model Updates & Versioning** | • Define version pinning strategy • Implement A/B testing for new versions • Create rollback procedures | Maintains stability while allowing improvements. |\n",
    "\n",
    "1. **Define Success Criteria Quantitatively:** Move beyond \"it works\" to measurable targets *before* major development.  \n",
    "     \n",
    "   * **Set Key Performance Indicators (KPIs) & SLOs:** Define specific targets for business value (e.g., RAG accuracy \\> 95%, OCR cost \\< $X/page) and performance (e.g., P95 latency \\< 1s, error rates).  \n",
    "   * **Ensure Measurability:** Confirm that all KPIs and SLOs can be directly measured from system logs (e.g., tracking `total_tokens`, `critique_status`).\n",
    "\n",
    "   \n",
    "\n",
    "2. **Document Initial Model Selection Rationale:** Justify your starting model choices for future reference.  \n",
    "     \n",
    "   * **Choose Models Deliberately:** Use the Model-Intro Matrix and use cases to select appropriate models for each task (e.g., `o4-mini` for speed/cost, `gpt-4.1` for accuracy, `o3` for depth).  \n",
    "   * **Record the \"Why\":** Briefly document the reasoning behind your choices (cost, latency, capability trade-offs) in code comments or design docs so future teams understand the context.\n",
    "\n",
    "   \n",
    "\n",
    "3. **Implement Robust Evaluation & Testing:** Verify quality and prevent regressions *before* shipping changes.  \n",
    "     \n",
    "   * **Build an Automated Eval Suite:** Create a repeatable test process using a \"golden set\" (50-100 diverse, expert-verified examples). Focus tests on `factuality`, `hallucination rate`, `tool-error rate`, and task-specific metrics.  \n",
    "   * **Test Reliably:** Rigorously test integrated tool reliability (success rate, error handling) and system behavior under load and with edge cases (malformed data, adversarial inputs).\n",
    "\n",
    "   \n",
    "\n",
    "4. **Establish Observability & Cost Controls:** Monitor performance and keep spending within budget.  \n",
    "     \n",
    "   * **Set Cost Guardrails:** Prevent unexpected cost increases by defining max token limits per stage and considering operational modes (\"Fast,\" \"Standard,\" \"Thorough\") to balance cost and performance.  \n",
    "   * **Implement Essential Logging:** Capture key operational data via structured logs for each processing stage to enable debugging and monitoring.\n",
    "\n",
    "   \n",
    "\n",
    "5. **Implement Safety & Compliance Guardrails:** Ensure responsible operation and meet requirements.  \n",
    "     \n",
    "   * **Use Safety Mechanisms:** Employ tools like OpenAI's moderation APIs, safety-focused system prompts, or sentinel models for checks, especially with user input or sensitive topics.  \n",
    "   * **Enforce Compliance:** Build in checks relevant to your specific industry and risks (e.g., legal constraints, lab safety).  \n",
    "   * **Require Human-in-the-Loop (HITL):** Mandate human review for low-confidence outputs, high-risk scenarios, or critical decisions, ensuring the workflow flags these items clearly.\n",
    "\n",
    "\n",
    "6. **Manage Model Updates and Versioning:** Prepare for model evolution over time.\n",
    "   \n",
    "   * **Version Pinning Strategy:** Decide whether to pin to specific model versions for stability or automatically adopt new versions for improvements.\n",
    "   * **A/B Testing Framework:** Establish a process to evaluate new model versions against your key metrics before full deployment.\n",
    "   * **Rollback Plan:** Create a clear procedure for reverting to previous model versions if issues arise with updates.\n",
    "   * **Monitor Version Performance:** Track metrics across model versions to identify performance trends and inform future selection decisions.\n",
    "\n",
    "================================================================================\n",
    "\n",
    "\n",
    "\n",
    "## Adaptation Decision Tree\n",
    "\n",
    "![Model Selection Decision Tree](../../../images/3D_model_selection_flowchart.png)\n",
    "\n",
    "## Communicating Model Selection to Non-Technical Stakeholders\n",
    "\n",
    "When explaining your model choices to business stakeholders, focus on these key points:\n",
    "\n",
    "1. **Align with Business Outcomes**: Explain how your model selection directly supports specific business goals (time savings, cost reduction, improved accuracy).\n",
    "\n",
    "2. **Translate Technical Metrics**: Convert technical considerations into business impact:\n",
    "   - \"This model reduces processing time from 5 seconds to 0.7 seconds, allowing us to handle customer inquiries 7x faster\"\n",
    "   - \"By using the mini variant, we can process 5x more documents within the same budget\"\n",
    "\n",
    "3. **Highlight Trade-offs**: Present clear scenarios for different models:\n",
    "   - \"Option A (GPT-4.1): Highest accuracy but higher cost - ideal for client-facing legal analysis\"\n",
    "   - \"Option B (GPT-4.1 mini): 90% of the accuracy at 30% of the cost - perfect for internal document processing\"\n",
    "\n",
    "4. **Use Concrete Examples**: Demonstrate the practical difference in outputs between models to illustrate the value proposition of each option.\n",
    "\n",
    "================================================================================\n",
    "\n",
    "\n",
    "\n",
    "## Appendices\n",
    "\n",
    "## Glossary of Key Terms\n",
    "\n",
    "| Term | Definition |\n",
    "|------|------------|\n",
    "| **Context Window** | The maximum number of tokens a model can process in a single request |\n",
    "| **Hallucination** | When a model generates content that appears plausible but is factually incorrect or unsupported |\n",
    "| **Latency** | The time delay between sending a request to a model and receiving a response |\n",
    "| **LLM** | Large Language Model; an AI system trained on vast amounts of text data |\n",
    "| **Prompt Engineering** | The practice of designing effective prompts to elicit desired outputs from AI models |\n",
    "| **RAG** | Retrieval-Augmented Generation; combining information retrieval with text generation |\n",
    "| **SOTA** | State-of-the-Art; representing the most advanced stage in a field at a given time |\n",
    "| **Token** | The basic unit of text that models process (roughly 0.75 words in English) |\n",
    "\n",
    "## 6.1 Price and Utility Table (Apr 2025)\n",
    "\n",
    "| Model | Context Window | Input Price (per 1M tokens) | Output Price (per 1M tokens) | Best For |\n",
    "|-------|----------------|-----------------------------|-----------------------------|----------|\n",
    "| GPT-4.1 | 1M | \\$2.00 | \\$8.00 | Long-doc analytics, code review |\n",
    "| GPT-4.1 mini | 1M | \\$0.40 | \\$1.60 | Production agents, balanced cost/performance |\n",
    "| GPT-4.1 nano | 1M | \\$0.10 | \\$0.40 | High-throughput, cost-sensitive applications |\n",
    "| GPT-4o | 128K | \\$5.00 | \\$15.00 | Real-time voice/vision chat |\n",
    "| GPT-4o mini | 128K | \\$0.15 | \\$0.60 | Vision tasks, rapid analytics |\n",
    "| o3 (low) | 200K | \\$10.00* | \\$40.00* | Bulk triage, catalog enrichment |\n",
    "| o3 (med) | 200K | \\$10.00* | \\$40.00* | Knowledge base Q&A |\n",
    "| o3 (high) | 200K | \\$10.00* | \\$40.00* | Multi-step reasoning, troubleshooting |\n",
    "| o4-mini (low) | 200K | \\$1.10* | \\$4.40* | Vision tasks, rapid analytics |\n",
    "| o4-mini (med) | 200K | \\$1.10* | \\$4.40* | Balanced vision + reasoning |\n",
    "| o4-mini (high) | 200K | \\$1.10* | \\$4.40* | Deep reasoning with cost control |\n",
    "\n",
    "\\* *Note: The low/med/high settings affect token usage rather than base pricing. Higher settings may use more tokens for deeper reasoning, increasing per-request cost and latency.*\n",
    "\n",
    "## 6.2 Prompt-pattern Quick Sheet (Token vs Latency Deltas)\n",
    "\n",
    "| Prompt Pattern | Description | Token Impact | Latency Impact | Best Model Fit |\n",
    "|----------------|-------------|--------------|----------------|----------------|\n",
    "| **Self-Critique** | Ask model to evaluate its own answer before finalizing | +20-30% tokens | +15-25% latency | GPT-4.1, o3 |\n",
    "| **Chain-of-Thought (CoT)** | Explicitly instruct to \"think step by step\" | +40-80% tokens | +30-50% latency | o3, o4-mini (high) |\n",
    "| **Structured Outputs** | Use JSON schema or pydantic models for consistent formatting | +5-10% tokens | +5-10% latency | All models |\n",
    "| **Zero-Token Memory** | Store context in external DB rather than in conversation | -70-90% tokens | -5-10% latency | GPT-4.1 family |\n",
    "| **Skeleton-Fill-In** | Provide template structure for model to complete | -10-20% tokens | -5-15% latency | o4-mini, GPT-4.1 nano |\n",
    "| **Self-Consistency** | Generate multiple answers and select most consistent | +200-300% tokens | +150-250% latency | o3 (high) |\n",
    "| **Role-Playing** | Assign specific personas to model for specialized knowledge | +5-15% tokens | Neutral | GPT-4o, o4-mini |\n",
    "| **Tournament Ranking** | Compare options pairwise rather than scoring individually | +50-100% tokens | +30-60% latency | o3, o4-mini (high) |\n",
    "| **Tool-Calling Reflex** | Prompt model to call tools when uncertainty is detected | +10-30% tokens | +20-40% latency | o3, GPT-4.1 |\n",
    "\n",
    "## 6.3 Links to External Cookbooks & Docs\n",
    "\n",
    "### OpenAI Official Resources\n",
    "- [OpenAI Cookbook Main Repository](https://cookbook.openai.com/)\n",
    "- [Function Calling Guide](https://platform.openai.com/docs/guides/function-calling)\n",
    "- [Vision Models Guide](https://platform.openai.com/docs/guides/vision)\n",
    "- [Agents Documentation](https://platform.openai.com/docs/guides/agents)\n",
    "- [Structured Outputs Guide](https://platform.openai.com/docs/guides/structured-outputs)\n",
    "\n",
    "### RAG & Retrieval\n",
    "- [RAG on PDFs](https://cookbook.openai.com/examples/file_search_responses)\n",
    "\n",
    "### Specialized Use Cases\n",
    "- [Voice Assistant with Agents SDK](https://cookbook.openai.com/examples/agents_sdk/app_assistant_voice_agents)\n",
    "- [Multi-Tool Orchestration](https://cookbook.openai.com/examples/responses_api/responses_api_tool_orchestration)\n",
    "- [Data Extraction and Transformation](https://cookbook.openai.com/examples/data_extraction_transformation)\n",
    "\n",
    "### Prompting & Model Selection\n",
    "- [GPT-4.1 Prompting Guide](https://cookbook.openai.com/examples/gpt4-1_prompting_guide)\n",
    "- [Prompt Engineering Best Practices](https://platform.openai.com/docs/guides/prompt-engineering)\n",
    "\n",
    "### Evaluation & Deployment\n",
    "- [Getting Started with OpenAI Evals](https://cookbook.openai.com/examples/evaluation/getting_started_with_openai_evals)\n",
    "- [How to use the Usage API and Cost API to monitor your OpenAI usage](https://cookbook.openai.com/examples/completions_usage_api)\n",
    "\n",
    "================================================================================\n",
    "\n",
    "\n",
    "\n",
    "## Contributors\n",
    "\n",
    " This cookbook serves as a joint collaboration effort between OpenAI and [Tribe AI](https://www.tribe.ai/)\n",
    "- [Kashyap Coimbatore Murali](https://www.linkedin.com/in/kashyap-murali/)\n",
    "- [Nate Harada](https://www.linkedin.com/in/nate-harada/) \n",
    "- [Sai Prashanth Soundararaj](https://www.linkedin.com/in/saiprashanths/)\n",
    "- [Shikhar Kwatra](https://www.linkedin.com/in/shikharkwatra/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
